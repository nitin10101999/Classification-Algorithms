# -*- coding: utf-8 -*-
"""DecisionTree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K2kPYDbAkyaKWP8Pt3U_xT0yN8rBtVkP
"""

#import os
#os.chdir('/content/drive/My Drive/Code/Ass3/Data')

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from scipy.stats import multivariate_normal
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn import tree
import copy 
import math
import csv
import time
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme(style="darkgrid")

# Default value
SSE = 0 
phi = -1
pi = 3.141593
dim = 1

def findSample(index):
  lst = df.loc[index]
  lst = lst.to_numpy().tolist()
  lst = lst[:len(lst)-1]
  return lst

class Node:
  def __init__(self,attribute, splitValue):
    self.attribute = attribute
    self.splitValue = splitValue
    self.parent = None
    self.rightChild = None
    self.leftChild = None

def split_data(x,y):
    X_train, X_test, y_train, y_test = train_test_split(
        x, y,stratify = y, test_size=0.2)
    print(len(X_train))
    y_tr = []
    y_tst = []
    y_tr.append(y_train)
    y_tst.append(y_test)
    train_data = np.concatenate([X_train,np.transpose(y_tr)],axis = 1)
    test_data = np.concatenate([X_test,np.transpose(y_tst)],axis = 1)

    return train_data,test_data

def getPartition(data,finalSplitPoint,finalScore,typ,finalatt):
  data_l = []
  data_r = []
  for index in range(len(data)):
    #if typ == 1:
    if (typ==1 and data[index][finalatt] < finalSplitPoint) or (typ ==0 and data[index][finalatt] == finalSplitPoint):
      data_l.append(data[index])
    else:
      data_r.append(data[index])
  return data_l,data_r

def entropy(data):
  entropy = 0
  for ele in data:
    try:
      entropy += -(ele)* math.log(ele,2)
    except:
      return 0
  return entropy

def Gain(Ny,n,P_D,P_DY,P_DN):
  Hd = entropy(P_D)
  H_Dy_Dn = (Ny/n)*entropy(P_DY) + ((n-Ny)/n)*entropy(P_DN)
  return Hd - H_Dy_Dn

def EvaluateNumericeAttribute(data,attribute,labels):
  #print('Numeric')
  try:
    data = data[np.argsort(data[:, attribute])]
  except:
    data.sort(key = lambda x: x[attribute])
  n = len(data)
  ni_s = {}
  M = set()
  N_vi = []
  for label in labels:
    ni_s[label] = 0
  for j in range(n-1):
    ni_s[data[j][dim-1]] += 1
    if data[j+1][attribute]  != data[j][attribute]:
      v = (data[j+1][attribute] + data[j][attribute])/2
      M.add(v)
      lst = []
      for label in labels: 
        lst.append(ni_s[label])
      N_vi.append(lst)
  ni_s[data[n-1][dim-1]] += 1
  v_final = None
  score_final = 0
  index = 0
  for v in M:
    P_D = []
    P_DY = []
    P_DN = []
    col = 0
    for label in labels:
      P_D.append(ni_s[label]/n)
      P_DY.append(N_vi[index][col]/sum(N_vi[index]))
      P_DN.append((ni_s[label]- N_vi[index][col])/(n -sum(N_vi[index])))
      col += 1
    score = Gain(sum(N_vi[index]),n,P_D,P_DY,P_DN)
    if score > score_final:
      v_final = v
      score_final = score
    index += 1
  return v_final,score_final

def EvaluateCategoricalAttribute(data,attribute,labels):
  #print("Categoric")
  n = len(data)
  ni_s = {}
  dom_X = {}
  mp_labels = {}
  N_vi = []
  index = 0
  for label in labels:
    mp_labels[label] = index
    ni_s[label] = 0
    index += 1
  
  index = 0
  for j in range(n):
    if data[j][attribute] in dom_X.keys():
      ni_s[data[j][dim-1]] += 1
    else:
      dom_X[data[j][attribute]] = index
      ni_s[data[j][dim-1]] = 1
      lst = [0]*len(labels)
      N_vi.append(lst)
      index += 1
  
  for j in range(n):
    ind = dom_X[data[j][attribute]]
    lab = mp_labels[data[j][dim-1]]
    N_vi[ind][lab] += 1 


  v_final = None
  score_final = 0
  index = 0
  for v in dom_X:
    P_D = []
    P_DY = []
    P_DN = []
    col = 0
    for label in labels:
      P_D.append(ni_s[label]/n)
      P_DY.append(N_vi[index][col]/sum(N_vi[index]))
      P_DN.append((ni_s[label]- N_vi[index][col])/(n -sum(N_vi[index])))
      col += 1
    score = Gain(sum(N_vi[index]),n,P_D,P_DY,P_DN)
    if score > score_final:
      v_final = v
      score_final = score
    index += 1
  return v_final,score_final

def buildTree(data, sizeThres, purityThres,types,labels,usedAttr):
  #print('here')
  n = len(data)
  if n == 0:
    return None
  ni_s = {}
  maxi = -1
  majClass = -1
  for index in range(n):
    ni_s.setdefault(data[index][dim-1], []).append(index)
    if len(ni_s[data[index][dim-1]]) > maxi:
      maxi = len(ni_s[data[index][dim-1]])
      majClass = data[index][dim-1]
  purityD = maxi/n
  #print(n,purityD,'------------------------------------------------')
  if n <= sizeThres or purityD >= purityThres:
    return majClass
  
  finalSplitPoint = None
  finalScore = 0
  finalatt = -1
  for atti in range(dim-1):
    if atti in usedAttr:
      continue
    if types[atti] == 1:  
      v, score = EvaluateNumericeAttribute(data,atti,labels)
      if score > finalScore:
        finalSplitPoint = v
        finalScore = score
        finalatt = atti
    elif types[atti] == 0: 
      v, score = EvaluateCategoricalAttribute(data,atti,labels)
      if score > finalScore:
        finalSplitPoint = v
        finalScore = score
        finalatt = atti
  data_l,data_r = getPartition(data,finalSplitPoint,finalScore,types[finalatt],finalatt)
  usedAttr.add(finalatt)
  #print(len(data),len(data_l),len(data_r))
  node = Node(finalSplitPoint,finalScore)
  node.leftChild = buildTree(data_l,sizeThres, purityThres,types,labels,usedAttr)
  print(len(data_r))
  node.rightChild = buildTree(data_r,sizeThres, purityThres,types,labels,usedAttr)
  print('4444444444444444444444444444444444444444')
  return node

def decisionTree(X,Y,depth):
  clf = tree.DecisionTreeClassifier(random_state=0, max_depth = depth)
  clf = clf.fit(X, Y)
  return clf

def fetchData(filename):
  df = pd.read_csv(filename)
  cols = np.array(df.columns)
  features = cols[:len(df.columns)]
  data = df.loc[:,features].values
  return data , df

def fetchData2(filename):
  df = pd.read_csv(filename)
  cols = np.array(df.columns)
  features = cols[:len(df.columns)-1]
  category = cols[len(df.columns)-1:]
  x = df.loc[:,features].values
  y = df.loc[:,category].values
  Y = []
  for i in range(len(y)):
    Y.append(y[i][0])
  return x , Y, df

def changeData(x,j):
  index = 0
  dic = {}
  for row in x:
    if not row[j] in dic.keys():
      dic[row[j]] = index
      index += 1
  for i in range(len(x)):
    x[i][j] = dic[x[i][j]]

def preProcessData(x,y,types):
  for i in range(len(types)):
    if types[i] == 0:
      changeData(x,i)
  for i in range(len(y)):
    if y[i] == ' <=50K':
      y[i] = 0
    else:
      y[i] = 1

def split_data(x,y):
    X_train, X_test, y_train, y_test = train_test_split(
        x, y,stratify = y, test_size=0.2)

    return  X_train, X_test, y_train, y_test

def testModel(X_test,clf):
  #Y_out = []
  return clf.predict(X_test)

def plotRelation(x,y):
	plt.figure(figsize=(8, 6),dpi=100)
	plt.xlabel('depth')
	plt.ylabel('Error Loss')
	plt.plot(x,y)
	plt.legend()

data ,df = fetchData('Data/adult.csv')
dim = len(df.columns)
types = []
for typ in df.dtypes:
  if typ == 'int64':
    types.append(1)
  else:
    types.append(0)
types = types[:dim-1]
labels = [' <=50K',' >50K']

usedAttr = set()
buildTree(data,40, 0.95,types,labels,usedAttr)


"""x, y, df = fetchData2('Data/adult.csv')
print(df.describe())
dim = len((df.columns))
types = []
for typ in df.dtypes:
  if typ == 'int64':
    types.append(1)
  else:
    types.append(0)
types = types[:dim-1]
labels = [' <=50K',' >50K']
preProcessData(x,y,types)

X_train, X_test, y_train, y_test = split_data(x,y)

depth = 2
maxDepths = []
loss = []
while depth <= 16:
  clf = decisionTree(X_train,y_train,depth) 
  Y_out = testModel(X_test,clf)
  loss.append(1-accuracy_score(Y_out,y_test))
  maxDepths.append(depth)
  depth += 2

print('loss:',loss)
print('depths:',maxDepths)
"""
"""fig = plt.figure(figsize=(25,20))
_ = tree.plot_tree(clf, 
                   feature_names=df.columns,  
                   class_names=labels,
                   filled=True)"""

#print(classification_report(y_test, Y_out, target_names=labels))

#print(accuracy_score(Y_out,y_test))

